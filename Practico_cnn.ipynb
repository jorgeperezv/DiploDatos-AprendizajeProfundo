{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gzip\n",
    "import json\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_data = \"./data/meli-challenge-2019/spanish.train.jsonl.gz\"\n",
    "_validation_data = \"./data/meli-challenge-2019/spanish.validation.jsonl.gz\"\n",
    "_test_data = \"./data/meli-challenge-2019/spanish.test.jsonl.gz\"\n",
    "_token_to_index = \"./data/meli-challenge-2019/spanish_token_to_index.json.gz\"\n",
    "_pretrained_embeddings = \"./data/SBW-vectors-300-min5.txt.gz\"\n",
    "_language = \"spanish\"\n",
    "_embeddings_size = 300\n",
    "_hidden_layers = [256, 128]\n",
    "_dropout = 0.3\n",
    "_epochs = 3\n",
    "\n",
    "_CNN_filters_count = 100\n",
    "_CNN_filters_length = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeliChallengeDataset(IterableDataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path,\n",
    "                 random_buffer_size=2048):\n",
    "        assert random_buffer_size > 0\n",
    "        self.dataset_path = dataset_path\n",
    "        self.random_buffer_size = random_buffer_size\n",
    "\n",
    "        with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
    "            item = json.loads(next(dataset).strip())\n",
    "            self.n_labels = item[\"n_labels\"]\n",
    "            self.dataset_size = item[\"size\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        try:\n",
    "            with gzip.open(self.dataset_path, \"rt\") as dataset:\n",
    "                shuffle_buffer = []\n",
    "\n",
    "                for line in dataset:\n",
    "                    item = json.loads(line.strip())\n",
    "                    item = {\n",
    "                        \"data\": item[\"data\"],\n",
    "                        \"target\": item[\"target\"]\n",
    "                    }\n",
    "\n",
    "                    if self.random_buffer_size == 1:\n",
    "                        yield item\n",
    "                    else:\n",
    "                        shuffle_buffer.append(item)\n",
    "\n",
    "                        if len(shuffle_buffer) == self.random_buffer_size:\n",
    "                            random.shuffle(shuffle_buffer)\n",
    "                            for item in shuffle_buffer:\n",
    "                                yield item\n",
    "                            shuffle_buffer = []\n",
    "\n",
    "                if len(shuffle_buffer) > 0:\n",
    "                    random.shuffle(shuffle_buffer)\n",
    "                    for item in shuffle_buffer:\n",
    "                        yield item\n",
    "        except GeneratorExit:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "            \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.LongTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una cuestión importante, las redes convolucionales sobre text esperan que todas las secuencias sean al menos del tamaño de la convolución máxima (caso contrario ocurrirá un error por no poder realizar la convolución sobre un espacio más chico que el tamaño de la convolución). Es por eso que utilizamos el parámetro min_length esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences(\n",
    "    pad_value=0,\n",
    "    max_length=None,\n",
    "    min_length=max(_CNN_filters_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building training dataset\n",
    "train_dataset = MeliChallengeDataset(\n",
    "    dataset_path=_train_data,\n",
    "    random_buffer_size=2048  # This can be a hypterparameter\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,  # This can be a hyperparameter\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_sequences,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building validation dataset\n",
    "validation_dataset = MeliChallengeDataset(\n",
    "    dataset_path=_validation_data,\n",
    "    random_buffer_size=1\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_sequences,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building test dataset\n",
    "test_dataset = MeliChallengeDataset(\n",
    "    dataset_path=_test_data,\n",
    "    random_buffer_size=1\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_sequences,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificación CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 token_to_index,\n",
    "                 n_labels,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        with gzip.open(token_to_index, \"rt\") as fh:\n",
    "            token_to_index = json.load(fh)\n",
    "        embeddings_matrix = torch.randn(len(token_to_index), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            next(fh)\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in token_to_index:\n",
    "                    embeddings_matrix[token_to_index[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.convs = []\n",
    "        for filter_lenght in _CNN_filters_length:\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(vector_size, _CNN_filters_count, filter_lenght)\n",
    "            )\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.fc = nn.Linear(_CNN_filters_count * len(_CNN_filters_length), 128)\n",
    "        self.output = nn.Linear(128, n_labels)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_global_max_pool(x, conv):\n",
    "        return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x).transpose(1, 2)  # Conv1d takes (batch, channel, seq_len)\n",
    "        x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(\n",
    "    pretrained_embeddings_path=_pretrained_embeddings, \n",
    "    token_to_index=_token_to_index,\n",
    "    n_labels=train_dataset.n_labels,\n",
    "    vector_size=_embeddings_size,\n",
    "    freeze_embedings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (embeddings): Embedding(50002, 300, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(300, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=632, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento de MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Práctico CNN' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6fb57f0dd544ada36d125323d7ba35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8be010a1484d8daad43c45d803a4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38245.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d16cc06cc394672b7ce19e3500db2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9562.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d008badbb4f24ae7b940d9b765acf892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38245.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdc4be54c1c409180b364b4fa746b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9562.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d42ed7b86b44c948cb1ce8c409e68be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38245.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceda125f2364a7cbcabf2a4c5c38975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9562.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc07a873fb343968c5c92d8533ee687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=498.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Práctico CNN\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log all relevent hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"Convolutional Network\",\n",
    "        \"embeddings\": _pretrained_embeddings,\n",
    "        \"dropout\": _dropout,\n",
    "        \"embeddings_size\": _embeddings_size,\n",
    "        \"freeze_embedding\": True,\n",
    "        \"epochs\": _epochs,\n",
    "        \"filters_count\": _CNN_filters_count,\n",
    "        \"filters_length\": _CNN_filters_length,\n",
    "        \"fc_size\": 128\n",
    "   \n",
    "    })\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,  # This can be a hyperparameter\n",
    "        weight_decay=1e-5  # This can be a hyperparameter\n",
    "    )\n",
    "\n",
    "    for epoch in trange(_epochs):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            data = batch[\"data\"].to(device)\n",
    "            target = batch[\"target\"].to(device)\n",
    "            output = model(data)\n",
    "            loss_value = loss(output, target)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())\n",
    "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "\n",
    "        if validation_dataset:\n",
    "            model.eval()\n",
    "            running_loss = []\n",
    "            targets = []\n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(validation_loader):\n",
    "                    data = batch[\"data\"].to(device)\n",
    "                    target = batch[\"target\"].to(device)\n",
    "                    output = model(data)\n",
    "                    running_loss.append(\n",
    "                        loss(output, target).item()\n",
    "                    )\n",
    "                    targets.extend(batch[\"target\"].numpy())\n",
    "                    predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
    "                mlflow.log_metric(\"validation_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "                mlflow.log_metric(\"validation_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n",
    "\n",
    "\n",
    "    if test_dataset:\n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader):\n",
    "                data = batch[\"data\"].to(device)\n",
    "                target = batch[\"target\"].to(device)\n",
    "                output = model(data)\n",
    "                running_loss.append(\n",
    "                    loss(output, target).item()\n",
    "                )\n",
    "                targets.extend(batch[\"target\"].numpy())\n",
    "                predictions.extend(output.argmax(axis=1).detach().cpu().numpy())\n",
    "            mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "            mlflow.log_metric(\"test_bacc\", balanced_accuracy_score(targets, predictions), epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
